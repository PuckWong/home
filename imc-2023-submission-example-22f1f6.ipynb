{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Baseline submission\n\nA notebook to generate a valid submission. Implements three local feature/matcher methods: LoFTR, DISK, and KeyNetAffNetHardNet.\n\nRemember to enable a GPU accelerator and disable internet access, then press \"submit\" on the right pane.","metadata":{}},{"cell_type":"code","source":"!pip install /kaggle/input/imc2023-dependencies-data/wheels/einops-0.6.1-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:04:31.088381Z","iopub.execute_input":"2023-05-27T16:04:31.089323Z","iopub.status.idle":"2023-05-27T16:05:05.741592Z","shell.execute_reply.started":"2023-05-27T16:04:31.089279Z","shell.execute_reply":"2023-05-27T16:05:05.740320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# General utilities\nimport os\nfrom tqdm import tqdm\nfrom time import time\nfrom fastprogress import progress_bar\nimport gc\nimport numpy as np\nimport h5py\nfrom IPython.display import clear_output\nfrom collections import defaultdict\nfrom copy import deepcopy\nimport pandas as pd\nfrom dataclasses import dataclass\n\n# CV/ML\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport kornia as K\nimport kornia.feature as KF\nfrom PIL import Image\nimport timm\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\nfrom torchvision import transforms\n# 3D reconstruction\nimport pycolmap","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:05:05.745111Z","iopub.execute_input":"2023-05-27T16:05:05.745824Z","iopub.status.idle":"2023-05-27T16:05:11.252124Z","shell.execute_reply.started":"2023-05-27T16:05:05.745766Z","shell.execute_reply":"2023-05-27T16:05:11.250519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device=torch.device('cuda')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append(\"../input/super-glue-pretrained-network\")\nfrom models.matching import Matching\nfrom models.utils import (compute_pose_error, compute_epipolar_error,\n                          estimate_pose, make_matching_plot,\n                          error_colormap, AverageTimer, pose_auc, read_image,\n                          rotate_intrinsics, rotate_pose_inplane,\n                          scale_intrinsics)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resize = [-1, ]\nresize_float = True\n\nconfig = {\n    \"superpoint\": {\n        \"nms_radius\": 8,\n        \"keypoint_threshold\": 0.005,\n        \"max_keypoints\": 2048\n    },\n    \"superglue\": {\n        \"weights\": \"outdoor\",\n        \"sinkhorn_iterations\": 50,\n        \"match_threshold\": 0.1,\n    }\n}\nsuper_glue_matcher = Matching(config).eval().to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#timm.list_models(pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:05:11.257364Z","iopub.execute_input":"2023-05-27T16:05:11.260318Z","iopub.status.idle":"2023-05-27T16:05:11.267430Z","shell.execute_reply.started":"2023-05-27T16:05:11.260262Z","shell.execute_reply":"2023-05-27T16:05:11.266236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Kornia version', K.__version__)\nprint('Pycolmap version', pycolmap.__version__)\n\nLOCAL_FEATURE = 'LoFTR' # 'KeyNetAffNetHardNet', 'LoFTR'\n\n# Can be LoFTR, KeyNetAffNetHardNet, or DISK","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:05:11.274362Z","iopub.execute_input":"2023-05-27T16:05:11.277395Z","iopub.status.idle":"2023-05-27T16:05:11.288188Z","shell.execute_reply.started":"2023-05-27T16:05:11.277353Z","shell.execute_reply":"2023-05-27T16:05:11.286983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEBUG_FLAG = False\nif len(os.listdir(\"/kaggle/input/image-matching-challenge-2023/test\")) < 2:\n    DEBUG_FLAG = True","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:05:11.292213Z","iopub.execute_input":"2023-05-27T16:05:11.293038Z","iopub.status.idle":"2023-05-27T16:05:11.302103Z","shell.execute_reply.started":"2023-05-27T16:05:11.292991Z","shell.execute_reply":"2023-05-27T16:05:11.301162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def arr_to_str(a):\n    return ';'.join([str(x) for x in a.reshape(-1)])\n\n\ndef load_torch_image(fname, device=torch.device('cpu')):\n    img = K.image_to_tensor(cv2.imread(fname), False).float() / 255.\n    img = K.color.bgr_to_rgb(img.to(device))\n    return img","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:05:11.304655Z","iopub.execute_input":"2023-05-27T16:05:11.306826Z","iopub.status.idle":"2023-05-27T16:05:11.314224Z","shell.execute_reply.started":"2023-05-27T16:05:11.306796Z","shell.execute_reply":"2023-05-27T16:05:11.312911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Code to manipulate a colmap database.\n# Forked from https://github.com/colmap/colmap/blob/dev/scripts/python/database.py\n\n# Copyright (c) 2018, ETH Zurich and UNC Chapel Hill.\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     * Redistributions of source code must retain the above copyright\n#       notice, this list of conditions and the following disclaimer.\n#\n#     * Redistributions in binary form must reproduce the above copyright\n#       notice, this list of conditions and the following disclaimer in the\n#       documentation and/or other materials provided with the distribution.\n#\n#     * Neither the name of ETH Zurich and UNC Chapel Hill nor the names of\n#       its contributors may be used to endorse or promote products derived\n#       from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n#\n# Author: Johannes L. Schoenberger (jsch-at-demuc-dot-de)\n\n# This script is based on an original implementation by True Price.\n\nimport sys\nimport sqlite3\nimport numpy as np\n\n\nIS_PYTHON3 = sys.version_info[0] >= 3\n\nMAX_IMAGE_ID = 2**31 - 1\n\nCREATE_CAMERAS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS cameras (\n    camera_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n    model INTEGER NOT NULL,\n    width INTEGER NOT NULL,\n    height INTEGER NOT NULL,\n    params BLOB,\n    prior_focal_length INTEGER NOT NULL)\"\"\"\n\nCREATE_DESCRIPTORS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS descriptors (\n    image_id INTEGER PRIMARY KEY NOT NULL,\n    rows INTEGER NOT NULL,\n    cols INTEGER NOT NULL,\n    data BLOB,\n    FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE)\"\"\"\n\nCREATE_IMAGES_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS images (\n    image_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n    name TEXT NOT NULL UNIQUE,\n    camera_id INTEGER NOT NULL,\n    prior_qw REAL,\n    prior_qx REAL,\n    prior_qy REAL,\n    prior_qz REAL,\n    prior_tx REAL,\n    prior_ty REAL,\n    prior_tz REAL,\n    CONSTRAINT image_id_check CHECK(image_id >= 0 and image_id < {}),\n    FOREIGN KEY(camera_id) REFERENCES cameras(camera_id))\n\"\"\".format(MAX_IMAGE_ID)\n\nCREATE_TWO_VIEW_GEOMETRIES_TABLE = \"\"\"\nCREATE TABLE IF NOT EXISTS two_view_geometries (\n    pair_id INTEGER PRIMARY KEY NOT NULL,\n    rows INTEGER NOT NULL,\n    cols INTEGER NOT NULL,\n    data BLOB,\n    config INTEGER NOT NULL,\n    F BLOB,\n    E BLOB,\n    H BLOB)\n\"\"\"\n\nCREATE_KEYPOINTS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS keypoints (\n    image_id INTEGER PRIMARY KEY NOT NULL,\n    rows INTEGER NOT NULL,\n    cols INTEGER NOT NULL,\n    data BLOB,\n    FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE)\n\"\"\"\n\nCREATE_MATCHES_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS matches (\n    pair_id INTEGER PRIMARY KEY NOT NULL,\n    rows INTEGER NOT NULL,\n    cols INTEGER NOT NULL,\n    data BLOB)\"\"\"\n\nCREATE_NAME_INDEX = \\\n    \"CREATE UNIQUE INDEX IF NOT EXISTS index_name ON images(name)\"\n\nCREATE_ALL = \"; \".join([\n    CREATE_CAMERAS_TABLE,\n    CREATE_IMAGES_TABLE,\n    CREATE_KEYPOINTS_TABLE,\n    CREATE_DESCRIPTORS_TABLE,\n    CREATE_MATCHES_TABLE,\n    CREATE_TWO_VIEW_GEOMETRIES_TABLE,\n    CREATE_NAME_INDEX\n])\n\n\ndef image_ids_to_pair_id(image_id1, image_id2):\n    if image_id1 > image_id2:\n        image_id1, image_id2 = image_id2, image_id1\n    return image_id1 * MAX_IMAGE_ID + image_id2\n\n\ndef pair_id_to_image_ids(pair_id):\n    image_id2 = pair_id % MAX_IMAGE_ID\n    image_id1 = (pair_id - image_id2) / MAX_IMAGE_ID\n    return image_id1, image_id2\n\n\ndef array_to_blob(array):\n    if IS_PYTHON3:\n        return array.tostring()\n    else:\n        return np.getbuffer(array)\n\n\ndef blob_to_array(blob, dtype, shape=(-1,)):\n    if IS_PYTHON3:\n        return np.fromstring(blob, dtype=dtype).reshape(*shape)\n    else:\n        return np.frombuffer(blob, dtype=dtype).reshape(*shape)\n\n\nclass COLMAPDatabase(sqlite3.Connection):\n\n    @staticmethod\n    def connect(database_path):\n        return sqlite3.connect(database_path, factory=COLMAPDatabase)\n\n\n    def __init__(self, *args, **kwargs):\n        super(COLMAPDatabase, self).__init__(*args, **kwargs)\n\n        self.create_tables = lambda: self.executescript(CREATE_ALL)\n        self.create_cameras_table = \\\n            lambda: self.executescript(CREATE_CAMERAS_TABLE)\n        self.create_descriptors_table = \\\n            lambda: self.executescript(CREATE_DESCRIPTORS_TABLE)\n        self.create_images_table = \\\n            lambda: self.executescript(CREATE_IMAGES_TABLE)\n        self.create_two_view_geometries_table = \\\n            lambda: self.executescript(CREATE_TWO_VIEW_GEOMETRIES_TABLE)\n        self.create_keypoints_table = \\\n            lambda: self.executescript(CREATE_KEYPOINTS_TABLE)\n        self.create_matches_table = \\\n            lambda: self.executescript(CREATE_MATCHES_TABLE)\n        self.create_name_index = lambda: self.executescript(CREATE_NAME_INDEX)\n\n    def add_camera(self, model, width, height, params,\n                   prior_focal_length=False, camera_id=None):\n        params = np.asarray(params, np.float64)\n        cursor = self.execute(\n            \"INSERT INTO cameras VALUES (?, ?, ?, ?, ?, ?)\",\n            (camera_id, model, width, height, array_to_blob(params),\n             prior_focal_length))\n        return cursor.lastrowid\n\n    def add_image(self, name, camera_id,\n                  prior_q=np.zeros(4), prior_t=np.zeros(3), image_id=None):\n        cursor = self.execute(\n            \"INSERT INTO images VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n            (image_id, name, camera_id, prior_q[0], prior_q[1], prior_q[2],\n             prior_q[3], prior_t[0], prior_t[1], prior_t[2]))\n        return cursor.lastrowid\n\n    def add_keypoints(self, image_id, keypoints):\n        assert(len(keypoints.shape) == 2)\n        assert(keypoints.shape[1] in [2, 4, 6])\n\n        keypoints = np.asarray(keypoints, np.float32)\n        self.execute(\n            \"INSERT INTO keypoints VALUES (?, ?, ?, ?)\",\n            (image_id,) + keypoints.shape + (array_to_blob(keypoints),))\n\n    def add_descriptors(self, image_id, descriptors):\n        descriptors = np.ascontiguousarray(descriptors, np.uint8)\n        self.execute(\n            \"INSERT INTO descriptors VALUES (?, ?, ?, ?)\",\n            (image_id,) + descriptors.shape + (array_to_blob(descriptors),))\n\n    def add_matches(self, image_id1, image_id2, matches):\n        assert(len(matches.shape) == 2)\n        assert(matches.shape[1] == 2)\n\n        if image_id1 > image_id2:\n            matches = matches[:,::-1]\n\n        pair_id = image_ids_to_pair_id(image_id1, image_id2)\n        matches = np.asarray(matches, np.uint32)\n        self.execute(\n            \"INSERT INTO matches VALUES (?, ?, ?, ?)\",\n            (pair_id,) + matches.shape + (array_to_blob(matches),))\n\n    def add_two_view_geometry(self, image_id1, image_id2, matches,\n                              F=np.eye(3), E=np.eye(3), H=np.eye(3), config=2):\n        assert(len(matches.shape) == 2)\n        assert(matches.shape[1] == 2)\n\n        if image_id1 > image_id2:\n            matches = matches[:,::-1]\n\n        pair_id = image_ids_to_pair_id(image_id1, image_id2)\n        matches = np.asarray(matches, np.uint32)\n        F = np.asarray(F, dtype=np.float64)\n        E = np.asarray(E, dtype=np.float64)\n        H = np.asarray(H, dtype=np.float64)\n        self.execute(\n            \"INSERT INTO two_view_geometries VALUES (?, ?, ?, ?, ?, ?, ?, ?)\",\n            (pair_id,) + matches.shape + (array_to_blob(matches), config,\n             array_to_blob(F), array_to_blob(E), array_to_blob(H)))","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:05:11.316530Z","iopub.execute_input":"2023-05-27T16:05:11.317539Z","iopub.status.idle":"2023-05-27T16:05:11.352144Z","shell.execute_reply.started":"2023-05-27T16:05:11.317497Z","shell.execute_reply":"2023-05-27T16:05:11.351043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Code to interface DISK with Colmap.\n# Forked from https://github.com/cvlab-epfl/disk/blob/37f1f7e971cea3055bb5ccfc4cf28bfd643fa339/colmap/h5_to_db.py\n\n#  Copyright [2020] [Michał Tyszkiewicz, Pascal Fua, Eduard Trulls]\n#\n#   Licensed under the Apache License, Version 2.0 (the \"License\");\n#   you may not use this file except in compliance with the License.\n#   You may obtain a copy of the License at\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#   Unless required by applicable law or agreed to in writing, software\n#   distributed under the License is distributed on an \"AS IS\" BASIS,\n#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#   See the License for the specific language governing permissions and\n#   limitations under the License.\n\nimport os, argparse, h5py, warnings\nimport numpy as np\nfrom tqdm import tqdm\nfrom PIL import Image, ExifTags\n\n\ndef get_focal(image_path, err_on_default=False):\n    image         = Image.open(image_path)\n    max_size      = max(image.size)\n\n    exif = image.getexif()\n    focal = None\n    if exif is not None:\n        focal_35mm = None\n        # https://github.com/colmap/colmap/blob/d3a29e203ab69e91eda938d6e56e1c7339d62a99/src/util/bitmap.cc#L299\n        for tag, value in exif.items():\n            focal_35mm = None\n            if ExifTags.TAGS.get(tag, None) == 'FocalLengthIn35mmFilm':\n                focal_35mm = float(value)\n                break\n\n        if focal_35mm is not None:\n            focal = focal_35mm / 35. * max_size\n    \n    if focal is None:\n        if err_on_default:\n            raise RuntimeError(\"Failed to find focal length\")\n\n        # failed to find it in exif, use prior\n        FOCAL_PRIOR = 1.2\n        focal = FOCAL_PRIOR * max_size\n\n    return focal\n\ndef create_camera(db, image_path, camera_model):\n    image         = Image.open(image_path)\n    width, height = image.size\n\n    focal = get_focal(image_path)\n\n    if camera_model == 'simple-pinhole':\n        model = 0 # simple pinhole\n        param_arr = np.array([focal, width / 2, height / 2])\n    if camera_model == 'pinhole':\n        model = 1 # pinhole\n        param_arr = np.array([focal, focal, width / 2, height / 2])\n    elif camera_model == 'simple-radial':\n        model = 2 # simple radial\n        param_arr = np.array([focal, width / 2, height / 2, 0.1])\n    elif camera_model == 'opencv':\n        model = 4 # opencv\n        param_arr = np.array([focal, focal, width / 2, height / 2, 0., 0., 0., 0.])\n         \n    return db.add_camera(model, width, height, param_arr)\n\n\ndef add_keypoints(db, h5_path, image_path, img_ext, camera_model, single_camera = True):\n    keypoint_f = h5py.File(os.path.join(h5_path, 'keypoints.h5'), 'r')\n\n    camera_id = None\n    fname_to_id = {}\n    for filename in tqdm(list(keypoint_f.keys())):\n        keypoints = keypoint_f[filename][()]\n\n        fname_with_ext = filename# + img_ext\n        path = os.path.join(image_path, fname_with_ext)\n        if not os.path.isfile(path):\n            raise IOError(f'Invalid image path {path}')\n\n        if camera_id is None or not single_camera:\n            camera_id = create_camera(db, path, camera_model)\n        image_id = db.add_image(fname_with_ext, camera_id)\n        fname_to_id[filename] = image_id\n\n        db.add_keypoints(image_id, keypoints)\n\n    return fname_to_id\n\ndef add_matches(db, h5_path, fname_to_id):\n    match_file = h5py.File(os.path.join(h5_path, 'matches.h5'), 'r')\n    \n    added = set()\n    n_keys = len(match_file.keys())\n    n_total = (n_keys * (n_keys - 1)) // 2\n\n    with tqdm(total=n_total) as pbar:\n        for key_1 in match_file.keys():\n            group = match_file[key_1]\n            for key_2 in group.keys():\n                id_1 = fname_to_id[key_1]\n                id_2 = fname_to_id[key_2]\n\n                pair_id = image_ids_to_pair_id(id_1, id_2)\n                if pair_id in added:\n                    warnings.warn(f'Pair {pair_id} ({id_1}, {id_2}) already added!')\n                    continue\n            \n                matches = group[key_2][()]\n                db.add_matches(id_1, id_2, matches)\n\n                added.add(pair_id)\n\n                pbar.update(1)","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:05:11.354056Z","iopub.execute_input":"2023-05-27T16:05:11.354452Z","iopub.status.idle":"2023-05-27T16:05:11.378594Z","shell.execute_reply.started":"2023-05-27T16:05:11.354415Z","shell.execute_reply":"2023-05-27T16:05:11.377547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We will use ViT global descriptor to get matching shortlists.\ndef get_global_desc(fnames, model, model2,\n                    device =  torch.device('cpu')):\n    model = model.eval()\n    model = model.to(device)\n    model2 = model2.eval()\n    model2 = model2.to(device)\n    config = resolve_data_config({}, model=model)\n    transform = transforms.Compose([\n                transforms.Resize(600, interpolation=transforms.InterpolationMode.BICUBIC),\n                transforms.CenterCrop(600),\n                transforms.ToTensor(),\n                transforms.Normalize([0.4850, 0.4560, 0.4060], [0.2290, 0.2240, 0.2250]),])#create_transform(**config)\n    global_descs_convnext=[]\n    for i, img_fname_full in tqdm(enumerate(fnames),total= len(fnames)):\n        key = os.path.splitext(os.path.basename(img_fname_full))[0]\n        img = Image.open(img_fname_full).convert('RGB')\n        timg = transform(img).unsqueeze(0).to(device)\n        with torch.no_grad():\n            desc = model.forward_features(timg.to(device)).mean(dim=(-1,2))#\n            #descf = model.forward_features(timg.to(device).flip(3)).mean(dim=(-1,2))#\n            desc2 = model2.forward_features(timg.to(device)).mean(dim=(-1,2))#\n            #desc2f = model2.forward_features(timg.to(device).flip(3)).mean(dim=(-1,2))#\n            #desc = desc+descf\n            #desc2 = desc2+desc2f\n            #print (desc.shape)\n            desc = desc.view(1, -1)\n            desc2 = desc2.view(1, -1)\n            desc_norm = torch.cat([desc, desc2], dim=-1)\n            desc_norm = F.normalize(desc_norm, dim=1, p=2)\n        #print (desc_norm)\n        global_descs_convnext.append(desc_norm.detach().cpu())\n    global_descs_all = torch.cat(global_descs_convnext, dim=0)\n    return global_descs_all\n\n\ndef get_img_pairs_exhaustive(img_fnames):\n    index_pairs = []\n    for i in range(len(img_fnames)):\n        for j in range(i+1, len(img_fnames)):\n            index_pairs.append((i,j))\n    return index_pairs\n\n\ndef get_image_pairs_shortlist(fnames,\n                              sim_th = 0.6, # should be strict\n                              min_pairs = 20,\n                              exhaustive_if_less = 20,\n                              device=torch.device('cpu')):\n    num_imgs = len(fnames)\n\n    if num_imgs <= exhaustive_if_less:\n        return get_img_pairs_exhaustive(fnames)\n\n    #model = timm.create_model('tf_efficientnet_b7',\n    #                          checkpoint_path='/kaggle/input/tf-efficientnet/pytorch/tf-efficientnet-b7/1/tf_efficientnet_b7_ra-6c08e654.pth')\n    \n    model.eval()\n    descs = get_global_desc(fnames, model, device=device)\n    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n    # removing half\n    mask = dm <= sim_th\n    total = 0\n    matching_list = []\n    ar = np.arange(num_imgs)\n    already_there_set = []\n    for st_idx in range(num_imgs-1):\n        mask_idx = mask[st_idx]\n        to_match = ar[mask_idx]\n        if len(to_match) > min_pairs:\n            to_match = np.argsort(dm[st_idx])[:min_pairs]  \n        for idx in to_match:\n            if st_idx == idx:\n                continue\n            if dm[st_idx, idx] < 1000:\n                matching_list.append(tuple(sorted((st_idx, idx.item()))))\n                total+=1\n    matching_list = sorted(list(set(matching_list)))\n    return matching_list\n\n\ndef get_image_pairs_shortlist_nn(fnames,\n                              exhaustive_if_less = 20,\n                              nneighbor=6,\n                              device=torch.device('cpu')):\n    num_imgs = len(fnames)\n    if num_imgs <= exhaustive_if_less or num_imgs <= nneighbor :\n        return get_img_pairs_exhaustive(fnames)\n\n    model = timm.create_model('tf_efficientnet_b7',\n                             checkpoint_path='/kaggle/input/tf-efficientnet/pytorch/tf-efficientnet-b7/1/tf_efficientnet_b7_ra-6c08e654.pth')\n    \n    model.eval()\n    model2 = timm.create_model('tf_efficientnet_b6_ns',\n                             checkpoint_path='/kaggle/input/imc-2023-timm-models/tf_efficientnet_b6_ns-51548356.pth')\n    \n    model2.eval()\n    descs = get_global_desc(fnames, model, model2, device=device)\n    dm = torch.einsum('bi,ki->bk', descs, descs).detach().cpu()\n    print(f\"sim shape: {dm.shape}\")\n    \n    val, indices = torch.topk(dm, k=nneighbor, dim=1)\n    if DEBUG_FLAG:\n        print(f\"sim: {val[:3]}\")\n        print(f\"nn: {indices[:3]}\")\n    matching_list = []\n    for st_idx in range(num_imgs-1):\n        for t in indices[st_idx]:\n            if t == st_idx:\n                continue\n            matching_list.append(tuple(sorted((st_idx, t.item()))))\n    matching_list = sorted(list(set(matching_list)))\n    return matching_list","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:05:11.380447Z","iopub.execute_input":"2023-05-27T16:05:11.381279Z","iopub.status.idle":"2023-05-27T16:05:11.407031Z","shell.execute_reply.started":"2023-05-27T16:05:11.381140Z","shell.execute_reply":"2023-05-27T16:05:11.405920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making kornia local features loading w/o internet\nclass KeyNetAffNetHardNet(KF.LocalFeature):\n    \"\"\"Convenience module, which implements KeyNet detector + AffNet + HardNet descriptor.\n\n    .. image:: _static/img/keynet_affnet.jpg\n    \"\"\"\n\n    def __init__(\n        self,\n        num_features: int = 5000,\n        upright: bool = False,\n        device = torch.device('cpu'),\n        scale_laf: float = 1.0,\n    ):\n        ori_module = KF.PassLAF() if upright else KF.LAFOrienter(angle_detector=KF.OriNet(False)).eval()\n        if not upright:\n            weights = torch.load('/kaggle/input/kornia-local-feature-weights/OriNet.pth')['state_dict']\n            ori_module.angle_detector.load_state_dict(weights)\n        detector = KF.KeyNetDetector(\n            False, num_features=num_features, ori_module=ori_module, aff_module=KF.LAFAffNetShapeEstimator(False).eval()\n        ).to(device)\n        kn_weights = torch.load('/kaggle/input/kornia-local-feature-weights/keynet_pytorch.pth')['state_dict']\n        detector.model.load_state_dict(kn_weights)\n        affnet_weights = torch.load('/kaggle/input/kornia-local-feature-weights/AffNet.pth')['state_dict']\n        detector.aff.load_state_dict(affnet_weights)\n        \n        hardnet = KF.HardNet(False).eval()\n        hn_weights = torch.load('/kaggle/input/kornia-local-feature-weights/HardNetLib.pth')['state_dict']\n        hardnet.load_state_dict(hn_weights)\n        descriptor = KF.LAFDescriptor(hardnet, patch_size=32, grayscale_descriptor=True).to(device)\n        super().__init__(detector, descriptor, scale_laf)","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:05:11.412423Z","iopub.execute_input":"2023-05-27T16:05:11.412760Z","iopub.status.idle":"2023-05-27T16:05:11.426170Z","shell.execute_reply.started":"2023-05-27T16:05:11.412731Z","shell.execute_reply":"2023-05-27T16:05:11.425130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def detect_features(img_fnames,\n                    num_feats = 2048,\n                    upright = False,\n                    device=torch.device('cpu'),\n                    feature_dir = '.featureout',\n                    resize_small_edge_to = 600):\n    if LOCAL_FEATURE == 'DISK':\n        # Load DISK from Kaggle models so it can run when the notebook is offline.\n        disk = KF.DISK().to(device)\n        pretrained_dict = torch.load('/kaggle/input/disk/pytorch/depth-supervision/1/loftr_outdoor.ckpt', map_location=device)\n        disk.load_state_dict(pretrained_dict['extractor'])\n        disk.eval()\n    if LOCAL_FEATURE == 'KeyNetAffNetHardNet':\n        feature = KeyNetAffNetHardNet(num_feats, upright, device).to(device).eval()\n    if not os.path.isdir(feature_dir):\n        os.makedirs(feature_dir)\n    with h5py.File(f'{feature_dir}/lafs.h5', mode='w') as f_laf, \\\n         h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp, \\\n         h5py.File(f'{feature_dir}/descriptors.h5', mode='w') as f_desc:\n        for img_path in progress_bar(img_fnames):\n            img_fname = img_path.split('/')[-1]\n            key = img_fname\n            with torch.inference_mode():\n                timg = load_torch_image(img_path, device=device)\n                H, W = timg.shape[2:]\n                if resize_small_edge_to is None:\n                    timg_resized = timg\n                else:\n                    timg_resized = K.geometry.resize(timg, resize_small_edge_to, antialias=True)\n                    print(f'Resized {timg.shape} to {timg_resized.shape} (resize_small_edge_to={resize_small_edge_to})')\n                h, w = timg_resized.shape[2:]\n                if LOCAL_FEATURE == 'DISK':\n                    features = disk(timg_resized, num_feats, pad_if_not_divisible=True)[0]\n                    kps1, descs = features.keypoints, features.descriptors\n                    \n                    lafs = KF.laf_from_center_scale_ori(kps1[None], torch.ones(1, len(kps1), 1, 1, device=device))\n                if LOCAL_FEATURE == 'KeyNetAffNetHardNet':\n                    lafs, resps, descs = feature(K.color.rgb_to_grayscale(timg_resized))\n                lafs[:,:,0,:] *= float(W) / float(w)\n                lafs[:,:,1,:] *= float(H) / float(h)\n                desc_dim = descs.shape[-1]\n                kpts = KF.get_laf_center(lafs).reshape(-1, 2).detach().cpu().numpy()\n                descs = descs.reshape(-1, desc_dim).detach().cpu().numpy()\n                f_laf[key] = lafs.detach().cpu().numpy()\n                f_kp[key] = kpts\n                f_desc[key] = descs\n    return\n\ndef get_unique_idxs(A, dim=0):\n    # https://stackoverflow.com/questions/72001505/how-to-get-unique-elements-and-their-firstly-appeared-indices-of-a-pytorch-tenso\n    unique, idx, counts = torch.unique(A, dim=dim, sorted=True, return_inverse=True, return_counts=True)\n    _, ind_sorted = torch.sort(idx, stable=True)\n    cum_sum = counts.cumsum(0)\n    cum_sum = torch.cat((torch.tensor([0],device=cum_sum.device), cum_sum[:-1]))\n    first_indices = ind_sorted[cum_sum]\n    return first_indices\n\ndef match_features(img_fnames,\n                   index_pairs,\n                   feature_dir = '.featureout',\n                   device=torch.device('cpu'),\n                   min_matches=15, \n                   force_mutual = True,\n                   matching_alg='smnn'\n                  ):\n    assert matching_alg in ['smnn', 'adalam']\n    with h5py.File(f'{feature_dir}/lafs.h5', mode='r') as f_laf, \\\n         h5py.File(f'{feature_dir}/descriptors.h5', mode='r') as f_desc, \\\n        h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n\n        for pair_idx in progress_bar(index_pairs):\n                    idx1, idx2 = pair_idx\n                    fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n                    key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n                    lafs1 = torch.from_numpy(f_laf[key1][...]).to(device)\n                    lafs2 = torch.from_numpy(f_laf[key2][...]).to(device)\n                    desc1 = torch.from_numpy(f_desc[key1][...]).to(device)\n                    desc2 = torch.from_numpy(f_desc[key2][...]).to(device)\n                    if matching_alg == 'adalam':\n                        img1, img2 = cv2.imread(fname1), cv2.imread(fname2)\n                        hw1, hw2 = img1.shape[:2], img2.shape[:2]\n                        adalam_config = KF.adalam.get_adalam_default_config()\n                        #adalam_config['orientation_difference_threshold'] = None\n                        #adalam_config['scale_rate_threshold'] = None\n                        adalam_config['force_seed_mnn']= False\n                        adalam_config['search_expansion'] = 16\n                        adalam_config['ransac_iters'] = 128\n                        adalam_config['device'] = device\n                        dists, idxs = KF.match_adalam(desc1, desc2,\n                                                      lafs1, lafs2, # Adalam takes into account also geometric information\n                                                      hw1=hw1, hw2=hw2,\n                                                      config=adalam_config) # Adalam also benefits from knowing image size\n                    else:\n                        dists, idxs = KF.match_smnn(desc1, desc2, 0.98)\n                    if len(idxs)  == 0:\n                        continue\n                    # Force mutual nearest neighbors\n                    if force_mutual:\n                        first_indices = get_unique_idxs(idxs[:,1])\n                        idxs = idxs[first_indices]\n                        dists = dists[first_indices]\n                    n_matches = len(idxs)\n                    if False:\n                        print (f'{key1}-{key2}: {n_matches} matches')\n                    group  = f_match.require_group(key1)\n                    if n_matches >= min_matches:\n                         group.create_dataset(key2, data=idxs.detach().cpu().numpy().reshape(-1, 2))\n    return","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:05:11.429165Z","iopub.execute_input":"2023-05-27T16:05:11.430193Z","iopub.status.idle":"2023-05-27T16:05:11.463815Z","shell.execute_reply.started":"2023-05-27T16:05:11.430026Z","shell.execute_reply":"2023-05-27T16:05:11.462767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_torch_image(fname, device, resize=True, img_max_size=1280):\n    img = cv2.imread(fname)\n    img_sz = img.shape\n    if img_max_size <=0:\n        resize = False\n    if resize:\n        scale = img_max_size / max(img.shape[0], img.shape[1]) \n    else:\n        scale = 1\n    w = int(img.shape[1] * scale)\n    h = int(img.shape[0] * scale)\n    img = cv2.resize(img, ((w//8)*8, (h//8)*8))\n    img = K.image_to_tensor(img, False).float() /255.\n    img = K.color.bgr_to_rgb(img)\n    return img.to(device), img_sz","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:05:11.466848Z","iopub.execute_input":"2023-05-27T16:05:11.469817Z","iopub.status.idle":"2023-05-27T16:05:11.478304Z","shell.execute_reply.started":"2023-05-27T16:05:11.469778Z","shell.execute_reply":"2023-05-27T16:05:11.477269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEVICE = device\nIMG_MAX_SIZE = [840, 640]\nDO_FLIP = False\nMAX_NUM_PAIRS = 6000","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:05:11.479962Z","iopub.execute_input":"2023-05-27T16:05:11.480334Z","iopub.status.idle":"2023-05-27T16:05:11.490924Z","shell.execute_reply.started":"2023-05-27T16:05:11.480295Z","shell.execute_reply":"2023-05-27T16:05:11.489903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def match_same_size(img_path0, img_path1, matcher, device=DEVICE, flip=False, img_max_size=1280):\n    img0, source_img_shape = load_torch_image(img_path0, device, img_max_size=img_max_size)\n    img1, target_img_shape = load_torch_image(img_path1, device, img_max_size=img_max_size)\n    input_dict = {\"image0\": K.color.rgb_to_grayscale(img0).to(device), \n                  \"image1\": K.color.rgb_to_grayscale(img1).to(device)}\n    if flip:\n        input_dict[\"image0\"] = torch.cat((input_dict[\"image0\"], torch.flip(input_dict[\"image0\"], dims=[3])), dim=0)\n        input_dict[\"image1\"] = torch.cat((input_dict[\"image1\"], torch.flip(input_dict[\"image1\"], dims=[3])), dim=0)\n        \n        \n    with torch.no_grad():\n        correspondences = matcher(input_dict)\n    \n    # flip coordinate\n    # https://github.com/kornia/kornia/blob/master/kornia/feature/loftr/loftr.py\n    keypoints0 = correspondences[\"keypoints0\"].cpu().numpy()\n    keypoints1 = correspondences[\"keypoints1\"].cpu().numpy()\n    confidence = correspondences[\"confidence\"].cpu().numpy()\n    batch_indexes = correspondences[\"batch_indexes\"].cpu().numpy()\n    if flip:\n        select_fliped = (batch_indexes == 1)\n        keypoints0[select_fliped, 0] = input_dict[\"image0\"].shape[-1] - keypoints0[select_fliped, 0]\n        keypoints1[select_fliped, 0] = input_dict[\"image1\"].shape[-1] - keypoints1[select_fliped, 0]\n    \n    # scale\n    new_source_img_shape = input_dict[\"image0\"].shape[-2:]\n    new_target_img_shape = input_dict[\"image1\"].shape[-2:]\n    sy, sx = source_img_shape[0] / new_source_img_shape[0], source_img_shape[1] / new_source_img_shape[1]\n    keypoints0[:, 0] = keypoints0[:, 0] * sx\n    keypoints0[:, 1] = keypoints0[:, 1] * sy\n\n    sy, sx = target_img_shape[0] / new_target_img_shape[0], target_img_shape[1] / new_target_img_shape[1]\n    keypoints1[:, 0] = keypoints1[:, 0] * sx\n    keypoints1[:, 1] = keypoints1[:, 1] * sy\n    torch.cuda.empty_cache()\n    return {\n        \"keypoints0\": keypoints0,\n        \"keypoints1\":  keypoints1,\n        \"confidence\": confidence\n    }","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:05:11.492530Z","iopub.execute_input":"2023-05-27T16:05:11.493387Z","iopub.status.idle":"2023-05-27T16:05:11.510024Z","shell.execute_reply.started":"2023-05-27T16:05:11.493348Z","shell.execute_reply":"2023-05-27T16:05:11.508918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def match_func(img_path0, img_path1, matcher, device=DEVICE):\n    mkpts0 = []\n    mkpts1 = []\n    confidence = []\n    for img_size in IMG_MAX_SIZE:\n        preds =  match_same_size(img_path0, img_path1, matcher, device=DEVICE, flip=DO_FLIP, img_max_size=img_size)\n        mkpts0.append(preds[\"keypoints0\"])\n        mkpts1.append(preds[\"keypoints1\"])\n        confidence.append(preds[\"confidence\"])\n    # concatenate\n    mkpts0 = np.concatenate(mkpts0, axis=0)\n    mkpts1 = np.concatenate(mkpts1, axis=0)\n    confidence = np.concatenate(confidence, axis=0)\n    \n    # filter pairs\n    ind = np.argsort(-confidence)\n    ind = ind[:MAX_NUM_PAIRS]\n    mkpts0 = mkpts0[ind]\n    mkpts1 = mkpts1[ind]\n    confidence = confidence[ind]\n    return mkpts0, mkpts1","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:05:11.511619Z","iopub.execute_input":"2023-05-27T16:05:11.512606Z","iopub.status.idle":"2023-05-27T16:05:11.523430Z","shell.execute_reply.started":"2023-05-27T16:05:11.512567Z","shell.execute_reply":"2023-05-27T16:05:11.522383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nfrom copy import deepcopy\nsys.path.append('/kaggle/input/imc2023-dependencies-data/DKM/')","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:05:11.524625Z","iopub.execute_input":"2023-05-27T16:05:11.525591Z","iopub.status.idle":"2023-05-27T16:05:11.533604Z","shell.execute_reply.started":"2023-05-27T16:05:11.525547Z","shell.execute_reply":"2023-05-27T16:05:11.532753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir -p pretrained/checkpoints\n!cp /kaggle/input/imc2023-dependencies-data/pretrained/dkm.pth pretrained/checkpoints/dkm_base_v11.pth\n\n!pip install -f /kaggle/input/imc2023-dependencies-data/wheels --no-index einops\n!cp -r /kaggle/input/imc2023-dependencies-data/DKM/ /kaggle/working/DKM/\n!cd /kaggle/working/DKM/; pip install -f /kaggle/input/imc2023-dependencies-data/wheels -e . ","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:05:11.534627Z","iopub.execute_input":"2023-05-27T16:05:11.536088Z","iopub.status.idle":"2023-05-27T16:06:03.250377Z","shell.execute_reply.started":"2023-05-27T16:05:11.536049Z","shell.execute_reply":"2023-05-27T16:06:03.249075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.hub.set_dir('/kaggle/working/pretrained/')\nfrom dkm import dkm_base\nDKM_matcher = dkm_base(pretrained=True, version=\"v11\").to(device).eval()","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:06:03.253433Z","iopub.execute_input":"2023-05-27T16:06:03.254321Z","iopub.status.idle":"2023-05-27T16:06:07.231125Z","shell.execute_reply.started":"2023-05-27T16:06:03.254273Z","shell.execute_reply":"2023-05-27T16:06:07.230040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def match_loftr(img_fnames,\n                   index_pairs,\n                   feature_dir = '.featureout_loftr',\n                   device=torch.device('cpu'),\n                   min_matches=15, resize_to_ = (640, 480)):\n    LoFTR_cfg = {'backbone_type': 'ResNetFPN',\n               'resolution': (8, 2),\n               'fine_window_size': 5,\n               'fine_concat_coarse_feat': True,\n               'resnetfpn': {'initial_dim': 128, 'block_dims': [128, 196, 256]},\n               'coarse': {'d_model': 256,\n                          'd_ffn': 256,\n                          'nhead': 8,\n                          'layer_names': ['self',\n                                          'cross',\n                                          'self',\n                                          'cross',\n                                          'self',\n                                          'cross',\n                                          'self',\n                                          'cross'],\n                          'attention': 'linear',\n                          'temp_bug_fix': False},\n               'match_coarse': {'thr': 0.2,\n                                'border_rm': 4,\n                                'match_type': 'dual_softmax',\n                                'dsmax_temperature': 0.1,\n                                'skh_iters': 3,\n                                'skh_init_bin_score': 1.0,\n                                'skh_prefilter': True,\n                                'train_coarse_percent': 0.4,\n                                'train_pad_num_gt_min': 200},\n               'fine': {'d_model': 128,\n                        'd_ffn': 128,\n                        'nhead': 8,\n                        'layer_names': ['self', 'cross'],\n                        'attention': 'linear'}}\n    #matcher = KF.LoFTR(pretrained=None)\n    #matcher = KF.LoFTR(pretrained=None, config=LoFTR_cfg)\n    #matcher.load_state_dict(torch.load('/kaggle/input/loftr/pytorch/outdoor/1/loftr_outdoor.ckpt')['state_dict'])\n    #matcher = matcher.to(device).eval()\n\n    # First we do pairwise matching, and then extract \"keypoints\" from loftr matches.\n    with h5py.File(f'{feature_dir}/matches_loftr.h5', mode='w') as f_match:\n        for pair_idx in progress_bar(index_pairs):\n            idx1, idx2 = pair_idx\n            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n            \n            #mkpts0, mkpts1 = match_func(fname1, fname2, matcher)\n            #torch.cuda.empty_cache()\n            image_1, inp_1, scales_1 = read_image(fname1, device, [-1,], 0, resize_float)\n            image_2, inp_2, scales_2 = read_image(fname2, device, [-1,], 0, resize_float)\n\n            pred = super_glue_matcher({\"image0\": inp_1, \"image1\": inp_2})\n            pred = {k: v[0].detach().cpu().numpy() for k, v in pred.items()}\n            kpts1, kpts2 = pred[\"keypoints0\"], pred[\"keypoints1\"]\n            matches, conf = pred[\"matches0\"], pred[\"matching_scores0\"]\n\n            valid = matches > -1\n            mkpts0 = kpts1[valid]\n            mkpts1 = kpts2[matches[valid]]\n            mconf = conf[valid]\n\n            n_matches = len(mkpts1)\n            group  = f_match.require_group(key1)\n            if n_matches >= min_matches:\n                 group.create_dataset(key2, data=np.concatenate([mkpts0, mkpts1], axis=1))\n\n    # Let's find unique loftr pixels and group them together.\n    kpts = defaultdict(list)\n    match_indexes = defaultdict(dict)\n    total_kpts=defaultdict(int)\n    with h5py.File(f'{feature_dir}/matches_loftr.h5', mode='r') as f_match:\n        for k1 in f_match.keys():\n            group  = f_match[k1]\n            for k2 in group.keys():\n                matches = group[k2][...]\n                total_kpts[k1]\n                kpts[k1].append(matches[:, :2])\n                kpts[k2].append(matches[:, 2:])\n                current_match = torch.arange(len(matches)).reshape(-1, 1).repeat(1, 2)\n                current_match[:, 0]+=total_kpts[k1]\n                current_match[:, 1]+=total_kpts[k2]\n                total_kpts[k1]+=len(matches)\n                total_kpts[k2]+=len(matches)\n                match_indexes[k1][k2]=current_match\n\n    for k in kpts.keys():\n        kpts[k] = np.round(np.concatenate(kpts[k], axis=0))\n    unique_kpts = {}\n    unique_match_idxs = {}\n    out_match = defaultdict(dict)\n    for k in kpts.keys():\n        uniq_kps, uniq_reverse_idxs = torch.unique(torch.from_numpy(kpts[k]),dim=0, return_inverse=True)\n        unique_match_idxs[k] = uniq_reverse_idxs\n        unique_kpts[k] = uniq_kps.numpy()\n    for k1, group in match_indexes.items():\n        for k2, m in group.items():\n            m2 = deepcopy(m)\n            m2[:,0] = unique_match_idxs[k1][m2[:,0]]\n            m2[:,1] = unique_match_idxs[k2][m2[:,1]]\n            mkpts = np.concatenate([unique_kpts[k1][ m2[:,0]],\n                                    unique_kpts[k2][  m2[:,1]],\n                                   ],\n                                   axis=1)\n            unique_idxs_current = get_unique_idxs(torch.from_numpy(mkpts), dim=0)\n            m2_semiclean = m2[unique_idxs_current]\n            unique_idxs_current1 = get_unique_idxs(m2_semiclean[:, 0], dim=0)\n            m2_semiclean = m2_semiclean[unique_idxs_current1]\n            unique_idxs_current2 = get_unique_idxs(m2_semiclean[:, 1], dim=0)\n            m2_semiclean2 = m2_semiclean[unique_idxs_current2]\n            out_match[k1][k2] = m2_semiclean2.numpy()\n    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp:\n        for k, kpts1 in unique_kpts.items():\n            f_kp[k] = kpts1\n    \n    with h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n        for k1, gr in out_match.items():\n            group  = f_match.require_group(k1)\n            for k2, match in gr.items():\n                group[k2] = match\n    return","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:06:07.232991Z","iopub.execute_input":"2023-05-27T16:06:07.233440Z","iopub.status.idle":"2023-05-27T16:06:07.269017Z","shell.execute_reply.started":"2023-05-27T16:06:07.233398Z","shell.execute_reply":"2023-05-27T16:06:07.268037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def import_into_colmap(img_dir,\n                       feature_dir ='.featureout',\n                       database_path = 'colmap.db',\n                       img_ext='.jpg'):\n    db = COLMAPDatabase.connect(database_path)\n    db.create_tables()\n    single_camera = False\n    fname_to_id = add_keypoints(db, feature_dir, img_dir, img_ext, 'simple-radial', single_camera)\n    add_matches(\n        db,\n        feature_dir,\n        fname_to_id,\n    )\n\n    db.commit()\n    return","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:06:07.270919Z","iopub.execute_input":"2023-05-27T16:06:07.271332Z","iopub.status.idle":"2023-05-27T16:06:07.278751Z","shell.execute_reply.started":"2023-05-27T16:06:07.271283Z","shell.execute_reply":"2023-05-27T16:06:07.277621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def construct_local_validation():\n    train_label_csv = \"/kaggle/input/image-matching-challenge-2023/train/train_labels.csv\"\n    df = pd.read_csv(train_label_csv).head(100)\n    df = df[[\"image_path\", \"dataset\", \"scene\", \"rotation_matrix\", \"translation_vector\"]]\n    df.to_csv(\"/kaggle/working/sample_submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:06:07.280248Z","iopub.execute_input":"2023-05-27T16:06:07.281254Z","iopub.status.idle":"2023-05-27T16:06:07.288311Z","shell.execute_reply.started":"2023-05-27T16:06:07.281215Z","shell.execute_reply":"2023-05-27T16:06:07.287095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from https://www.kaggle.com/code/eduardtrulls/imc2023-evaluation\n# Evaluation metric.\n\n@dataclass\nclass Camera:\n    rotmat: np.array\n    tvec: np.array\n\ndef quaternion_from_matrix(matrix):\n    M = np.array(matrix, dtype=np.float64, copy=False)[:4, :4]\n    m00 = M[0, 0]\n    m01 = M[0, 1]\n    m02 = M[0, 2]\n    m10 = M[1, 0]\n    m11 = M[1, 1]\n    m12 = M[1, 2]\n    m20 = M[2, 0]\n    m21 = M[2, 1]\n    m22 = M[2, 2]\n\n    # Symmetric matrix K.\n    K = np.array([[m00 - m11 - m22, 0.0, 0.0, 0.0],\n                  [m01 + m10, m11 - m00 - m22, 0.0, 0.0],\n                  [m02 + m20, m12 + m21, m22 - m00 - m11, 0.0],\n                  [m21 - m12, m02 - m20, m10 - m01, m00 + m11 + m22]])\n    K /= 3.0\n\n    # Quaternion is eigenvector of K that corresponds to largest eigenvalue.\n    w, V = np.linalg.eigh(K)\n    q = V[[3, 0, 1, 2], np.argmax(w)]\n\n    if q[0] < 0.0:\n        np.negative(q, q)\n    return q\n\ndef evaluate_R_t(R_gt, t_gt, R, t, eps=1e-15):\n    t = t.flatten()\n    t_gt = t_gt.flatten()\n\n    q_gt = quaternion_from_matrix(R_gt)\n    q = quaternion_from_matrix(R)\n    q = q / (np.linalg.norm(q) + eps)\n    q_gt = q_gt / (np.linalg.norm(q_gt) + eps)\n    loss_q = np.maximum(eps, (1.0 - np.sum(q * q_gt)**2))\n    err_q = np.arccos(1 - 2 * loss_q)\n\n    GT_SCALE = np.linalg.norm(t_gt)\n    t = GT_SCALE * (t / (np.linalg.norm(t) + eps))\n    err_t = min(np.linalg.norm(t_gt - t), np.linalg.norm(t_gt + t))\n    \n    return np.degrees(err_q), err_t\n\ndef compute_dR_dT(R1, T1, R2, T2):\n    '''Given absolute (R, T) pairs for two cameras, compute the relative pose difference, from the first.'''\n    \n    dR = np.dot(R2, R1.T)\n    dT = T2 - np.dot(dR, T1)\n    return dR, dT\n\ndef compute_mAA(err_q, err_t, ths_q, ths_t):\n    '''Compute the mean average accuracy over a set of thresholds. Additionally returns the metric only over rotation and translation.'''\n\n    acc, acc_q, acc_t = [], [], []\n    for th_q, th_t in zip(ths_q, ths_t):\n        cur_acc_q = (err_q <= th_q)\n        cur_acc_t = (err_t <= th_t)\n        cur_acc = cur_acc_q & cur_acc_t\n        \n        acc.append(cur_acc.astype(np.float32).mean())\n        acc_q.append(cur_acc_q.astype(np.float32).mean())\n        acc_t.append(cur_acc_t.astype(np.float32).mean())\n    return np.array(acc), np.array(acc_q), np.array(acc_t)\n\ndef dict_from_csv(csv_path, has_header):\n    csv_dict = {}\n    with open(csv_path, 'r') as f:\n        for i, l in enumerate(f):\n            if has_header and i == 0:\n                continue\n            if l:\n                image, dataset, scene, R_str, T_str = l.strip().split(',')\n                R = np.fromstring(R_str.strip(), sep=';').reshape(3, 3)\n                T = np.fromstring(T_str.strip(), sep=';')\n                if dataset not in csv_dict:\n                    csv_dict[dataset] = {}\n                if scene not in csv_dict[dataset]:\n                    csv_dict[dataset][scene] = {}\n                csv_dict[dataset][scene][image] = Camera(rotmat=R, tvec=T)\n    return csv_dict\n\ndef eval_submission(submission_csv_path, ground_truth_csv_path, rotation_thresholds_degrees_dict, translation_thresholds_meters_dict, verbose=False):\n    '''Compute final metric given submission and ground truth files. Thresholds are specified per dataset.'''\n\n    submission_dict = dict_from_csv(submission_csv_path, has_header=True)\n    gt_dict = dict_from_csv(ground_truth_csv_path, has_header=True)\n\n    # Check that all necessary keys exist in the submission file\n    for dataset in gt_dict:\n        assert dataset in submission_dict, f'Unknown dataset: {dataset}'\n        for scene in gt_dict[dataset]:\n            assert scene in submission_dict[dataset], f'Unknown scene: {dataset}->{scene}'\n            for image in gt_dict[dataset][scene]:\n                assert image in submission_dict[dataset][scene], f'Unknown image: {dataset}->{scene}->{image}'\n\n    # Iterate over all the scenes\n    if verbose:\n        t = time()\n        print('*** METRICS ***')\n\n    metrics_per_dataset = []\n    for dataset in gt_dict:\n        metrics_per_scene = []\n        for scene in gt_dict[dataset]:\n            err_q_all = []\n            err_t_all = []\n            images = [camera for camera in gt_dict[dataset][scene]]\n            # Process all pairs in a scene\n            for i in range(len(images)):\n                for j in range(i + 1, len(images)):\n                    gt_i = gt_dict[dataset][scene][images[i]]\n                    gt_j = gt_dict[dataset][scene][images[j]]\n                    dR_gt, dT_gt = compute_dR_dT(gt_i.rotmat, gt_i.tvec, gt_j.rotmat, gt_j.tvec)\n\n                    pred_i = submission_dict[dataset][scene][images[i]]\n                    pred_j = submission_dict[dataset][scene][images[j]]\n                    dR_pred, dT_pred = compute_dR_dT(pred_i.rotmat, pred_i.tvec, pred_j.rotmat, pred_j.tvec)\n\n                    err_q, err_t = evaluate_R_t(dR_gt, dT_gt, dR_pred, dT_pred)\n                    err_q_all.append(err_q)\n                    err_t_all.append(err_t)\n\n            mAA, mAA_q, mAA_t = compute_mAA(err_q=err_q_all,\n                                            err_t=err_t_all,\n                                            ths_q=rotation_thresholds_degrees_dict[(dataset, scene)],\n                                            ths_t=translation_thresholds_meters_dict[(dataset, scene)])\n            if verbose:\n                print(f'{dataset} / {scene} ({len(images)} images, {len(err_q_all)} pairs) -> mAA={np.mean(mAA):.06f}, mAA_q={np.mean(mAA_q):.06f}, mAA_t={np.mean(mAA_t):.06f}')\n            metrics_per_scene.append(np.mean(mAA))\n\n        metrics_per_dataset.append(np.mean(metrics_per_scene))\n        if verbose:\n            print(f'{dataset} -> mAA={np.mean(metrics_per_scene):.06f}')\n            print()\n\n    if verbose:\n        print(f'Final metric -> mAA={np.mean(metrics_per_dataset):.06f} (t: {time() - t} sec.)')\n        print()\n\n    return np.mean(metrics_per_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:06:07.289924Z","iopub.execute_input":"2023-05-27T16:06:07.290391Z","iopub.status.idle":"2023-05-27T16:06:07.328424Z","shell.execute_reply.started":"2023-05-27T16:06:07.290352Z","shell.execute_reply":"2023-05-27T16:06:07.327373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set rotation thresholds per scene.\nrotation_thresholds_degrees_dict = {\n    **{('haiper', scene): np.linspace(1, 10, 10) for scene in ['bike', 'chairs', 'fountain']},\n    **{('heritage', scene): np.linspace(1, 10, 10) for scene in ['cyprus', 'dioscuri']},\n    **{('heritage', 'wall'): np.linspace(0.2, 10, 10)},\n    **{('urban', 'kyiv-puppet-theater'): np.linspace(1, 10, 10)},\n}\n\ntranslation_thresholds_meters_dict = {\n    **{('haiper', scene): np.geomspace(0.05, 0.5, 10) for scene in ['bike', 'chairs', 'fountain']},\n    **{('heritage', scene): np.geomspace(0.1, 2, 10) for scene in ['cyprus', 'dioscuri']},\n    **{('heritage', 'wall'): np.geomspace(0.05, 1, 10)},\n    **{('urban', 'kyiv-puppet-theater'): np.geomspace(0.5, 5, 10)},\n}","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:06:07.329736Z","iopub.execute_input":"2023-05-27T16:06:07.330125Z","iopub.status.idle":"2023-05-27T16:06:07.347919Z","shell.execute_reply.started":"2023-05-27T16:06:07.330087Z","shell.execute_reply":"2023-05-27T16:06:07.346802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"src = '/kaggle/input/image-matching-challenge-2023'\nif DEBUG_FLAG:\n    construct_local_validation()\n    sample_submission_file = f'/kaggle/working/sample_submission.csv'\n    img_base_dir = os.path.join(src, \"train\")\nelse:\n    sample_submission_file = f'/kaggle/input/image-matching-challenge-2023/sample_submission.csv'\n    img_base_dir = os.path.join(src, \"test\")","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:06:07.349520Z","iopub.execute_input":"2023-05-27T16:06:07.352372Z","iopub.status.idle":"2023-05-27T16:06:07.387087Z","shell.execute_reply.started":"2023-05-27T16:06:07.352331Z","shell.execute_reply":"2023-05-27T16:06:07.386146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get data from csv.\ndata_dict = {}\nwith open(sample_submission_file, 'r') as f:\n    for i, l in enumerate(f):\n        # Skip header.\n        if l and i > 0:\n            image, dataset, scene, _, _ = l.strip().split(',')\n            if dataset not in data_dict:\n                data_dict[dataset] = {}\n            if scene not in data_dict[dataset]:\n                data_dict[dataset][scene] = []\n            data_dict[dataset][scene].append(image)","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:06:07.388591Z","iopub.execute_input":"2023-05-27T16:06:07.388983Z","iopub.status.idle":"2023-05-27T16:06:07.396884Z","shell.execute_reply.started":"2023-05-27T16:06:07.388945Z","shell.execute_reply":"2023-05-27T16:06:07.395621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dataset in data_dict:\n    for scene in data_dict[dataset]:\n        print(f'{dataset} / {scene} -> {len(data_dict[dataset][scene])} images')","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:06:07.398858Z","iopub.execute_input":"2023-05-27T16:06:07.399805Z","iopub.status.idle":"2023-05-27T16:06:07.406811Z","shell.execute_reply.started":"2023-05-27T16:06:07.399766Z","shell.execute_reply":"2023-05-27T16:06:07.405662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out_results = {}\ntimings = {\"shortlisting\":[],\n           \"feature_detection\": [],\n           \"feature_matching\":[],\n           \"RANSAC\": [],\n           \"Reconstruction\": []}","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:06:07.408568Z","iopub.execute_input":"2023-05-27T16:06:07.409361Z","iopub.status.idle":"2023-05-27T16:06:07.415836Z","shell.execute_reply.started":"2023-05-27T16:06:07.409312Z","shell.execute_reply":"2023-05-27T16:06:07.414760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to create a submission file.\ndef create_submission(out_results, data_dict):\n    with open(f'submission.csv', 'w') as f:\n        f.write('image_path,dataset,scene,rotation_matrix,translation_vector\\n')\n        for dataset in data_dict:\n            if dataset in out_results:\n                res = out_results[dataset]\n            else:\n                res = {}\n            for scene in data_dict[dataset]:\n                if scene in res:\n                    scene_res = res[scene]\n                else:\n                    scene_res = {\"R\":{}, \"t\":{}}\n                for image in data_dict[dataset][scene]:\n                    if image in scene_res:\n                        print (image)\n                        R = scene_res[image]['R'].reshape(-1)\n                        T = scene_res[image]['t'].reshape(-1)\n                    else:\n                        R = np.eye(3).reshape(-1)\n                        T = np.zeros((3))\n                    f.write(f'{image},{dataset},{scene},{arr_to_str(R)},{arr_to_str(T)}\\n')","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:06:07.422447Z","iopub.execute_input":"2023-05-27T16:06:07.423155Z","iopub.status.idle":"2023-05-27T16:06:07.432659Z","shell.execute_reply.started":"2023-05-27T16:06:07.423122Z","shell.execute_reply":"2023-05-27T16:06:07.431638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\ndatasets = []\nfor dataset in data_dict:\n    datasets.append(dataset)\n\nfor dataset in datasets:\n    print(dataset)\n    if dataset not in out_results:\n        out_results[dataset] = {}\n    for scene in data_dict[dataset]:\n        print(scene)\n        # Fail gently if the notebook has not been submitted and the test data is not populated.\n        # You may want to run this on the training data in that case?\n        img_dir = f'{img_base_dir}/{dataset}/{scene}/images'\n        if not os.path.exists(img_dir):\n            continue\n        # Wrap the meaty part in a try-except block.\n        try:\n            out_results[dataset][scene] = {}\n            img_fnames = [f'{img_base_dir}/{x}' for x in data_dict[dataset][scene]]\n            print (f\"Got {len(img_fnames)} images\")\n            feature_dir = f'featureout/{dataset}_{scene}'\n            if not os.path.isdir(feature_dir):\n                os.makedirs(feature_dir, exist_ok=True)\n            \n            ## Pairs\n            t=time()\n            index_pairs =get_image_pairs_shortlist_nn(img_fnames,\n                                                    nneighbor=20,              \n                                                    exhaustive_if_less = 20,\n                                                    device=device)\n            t=time() -t \n            timings['shortlisting'].append(t)\n            print (f'{len(index_pairs)}, pairs to match, {t:.4f} sec')\n            gc.collect()\n            \n            # Desc & Matching\n            t=time()\n            if LOCAL_FEATURE != 'LoFTR':\n                detect_features(img_fnames, \n                                2048,\n                                feature_dir=feature_dir,\n                                upright=True,\n                                device=device,\n                                resize_small_edge_to=600\n                               )\n                gc.collect()\n                t=time() -t \n                timings['feature_detection'].append(t)\n                print(f'Features detected in  {t:.4f} sec')\n                t=time()\n                match_features(img_fnames, index_pairs, feature_dir=feature_dir,device=device, matching_alg='adalam')\n            else:\n                match_loftr(img_fnames, index_pairs, feature_dir=feature_dir, device=device, resize_to_=(600, 800))\n            t=time() -t \n            timings['feature_matching'].append(t)\n            print(f'Features matched in  {t:.4f} sec')\n            \n            ## Colmap\n            database_path = f'{feature_dir}/colmap.db'\n            if os.path.isfile(database_path):\n                os.remove(database_path)\n            gc.collect()\n            import_into_colmap(img_dir, feature_dir=feature_dir,database_path=database_path)\n            output_path = f'{feature_dir}/colmap_rec_{LOCAL_FEATURE}'\n\n            t=time()\n            pycolmap.match_exhaustive(database_path)\n            t=time() - t \n            timings['RANSAC'].append(t)\n            print(f'RANSAC in  {t:.4f} sec')\n\n            t=time()\n            # By default colmap does not generate a reconstruction if less than 10 images are registered. Lower it to 3.\n            mapper_options = pycolmap.IncrementalMapperOptions()\n            mapper_options.min_model_size = 3\n            os.makedirs(output_path, exist_ok=True)\n            maps = pycolmap.incremental_mapping(database_path=database_path, image_path=img_dir, output_path=output_path, options=mapper_options)\n            print(maps)\n            #clear_output(wait=False)\n            t=time() - t\n            timings['Reconstruction'].append(t)\n            print(f'Reconstruction done in  {t:.4f} sec')\n            \n            ## Export\n            imgs_registered  = 0\n            best_idx = None\n            print (\"Looking for the best reconstruction\")\n            if isinstance(maps, dict):\n                for idx1, rec in maps.items():\n                    print (idx1, rec.summary())\n                    if len(rec.images) > imgs_registered:\n                        imgs_registered = len(rec.images)\n                        best_idx = idx1\n                    for k, im in maps[idx1].images.items():\n                        key1 = f'{dataset}/{scene}/images/{im.name}'\n                        if key1 not in out_results[dataset][scene]:\n                            out_results[dataset][scene][key1] = {}\n                        out_results[dataset][scene][key1][\"R\"] = deepcopy(im.rotmat())\n                        out_results[dataset][scene][key1][\"t\"] = deepcopy(np.array(im.tvec))\n                    \n            if best_idx is not None:\n                print (maps[best_idx].summary())\n                for k, im in maps[best_idx].images.items():\n                    key1 = f'{dataset}/{scene}/images/{im.name}'\n                    if key1 not in out_results[dataset][scene]:\n                            out_results[dataset][scene][key1] = {}\n                    out_results[dataset][scene][key1][\"R\"] = deepcopy(im.rotmat())\n                    out_results[dataset][scene][key1][\"t\"] = deepcopy(np.array(im.tvec))\n            print(f'Registered: {dataset} / {scene} -> {len(out_results[dataset][scene])} images')\n            print(f'Total: {dataset} / {scene} -> {len(data_dict[dataset][scene])} images')\n            create_submission(out_results, data_dict)\n            gc.collect()\n        except Exception as e:\n            print(\"*\"*20)\n            print(e)","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:06:07.434279Z","iopub.execute_input":"2023-05-27T16:06:07.435032Z","iopub.status.idle":"2023-05-27T16:06:32.542195Z","shell.execute_reply.started":"2023-05-27T16:06:07.434993Z","shell.execute_reply":"2023-05-27T16:06:32.540154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_submission(out_results, data_dict)","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:06:32.544010Z","iopub.execute_input":"2023-05-27T16:06:32.544326Z","iopub.status.idle":"2023-05-27T16:06:32.551668Z","shell.execute_reply.started":"2023-05-27T16:06:32.544296Z","shell.execute_reply":"2023-05-27T16:06:32.550365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DEBUG_FLAG:\n    eval_submission(submission_csv_path='submission.csv',\n                ground_truth_csv_path='sample_submission.csv',\n                rotation_thresholds_degrees_dict=rotation_thresholds_degrees_dict,\n                translation_thresholds_meters_dict=translation_thresholds_meters_dict,\n                verbose=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-27T16:06:32.553317Z","iopub.execute_input":"2023-05-27T16:06:32.553995Z","iopub.status.idle":"2023-05-27T16:06:33.148905Z","shell.execute_reply.started":"2023-05-27T16:06:32.553955Z","shell.execute_reply":"2023-05-27T16:06:33.146803Z"},"trusted":true},"execution_count":null,"outputs":[]}]}